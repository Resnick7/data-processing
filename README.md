# üßæ **Proceso ETL y an√°lisis de datos de videojuegos en Databricks**

## **Introducci√≥n**

El presente trabajo pr√°ctico tiene como objetivo aplicar los conceptos fundamentales del procesamiento de datos masivos utilizando **Apache Spark** dentro del entorno **Databricks Community Edition**.
Se desarrolla un flujo **ETL (Extracci√≥n, Transformaci√≥n y Carga)** sobre un dataset p√∫blico de ventas de videojuegos, con el prop√≥sito de explorar, limpiar y analizar los datos de manera distribuida, y finalmente almacenar los resultados en formato **Delta Lake**, un est√°ndar de almacenamiento transaccional optimizado para entornos de big data.

El proyecto permite familiarizarse con el uso de **notebooks, clusters y operaciones con PySpark**, reforzando el v√≠nculo entre la teor√≠a del procesamiento distribuido y la pr√°ctica aplicada a un contexto real del mercado laboral relacionado con la **industria de los videojuegos**.


## **Configuraci√≥n del entorno**

Para el desarrollo se utiliz√≥ **Databricks Community Edition**, una plataforma en la nube que integra Apache Spark y ofrece un entorno interactivo para ejecutar c√≥digo en **Python, SQL, Scala o R**.

Las principales configuraciones fueron:

* Creaci√≥n de un **cluster** basado en Apache Spark.
* Configuraci√≥n de un **notebook** asociado al cluster.
* Importaci√≥n y lectura de datos mediante la librer√≠a PySpark.
* Exploraci√≥n inicial mediante comandos como `df.show()` y `df.printSchema()` para verificar la estructura y tipos de datos.

Esta configuraci√≥n permite aprovechar la **computaci√≥n distribuida**, en la que las tareas se paralelizan en varios nodos del cluster, incrementando la eficiencia y escalabilidad.

## **Ingesta de datos**

El dataset seleccionado fue **‚ÄúVideo Game Sales‚Äù (vgsales.csv)**, que contiene informaci√≥n sobre m√°s de 16.000 t√≠tulos, incluyendo su plataforma, g√©nero, desarrollador y ventas por regi√≥n.
El archivo se obtuvo desde un repositorio p√∫blico de GitHub y se carg√≥ al entorno de trabajo de Databricks mediante el comando:

```python
df = spark.read.option("header", True).csv("/Volumes/workspace/default/tmp_steam/vgsales.csv")
```

Luego, se visualizaron las primeras filas con:

```python
df.show(5)
```

y se inspeccion√≥ la estructura con:

```python
df.printSchema()
```

Esta etapa permiti√≥ confirmar la correcta lectura del archivo y el reconocimiento de los tipos de datos (string, double, etc.).
En caso de que las columnas num√©ricas fueran importadas como texto, se aplicaron conversiones de tipo con `cast()` para asegurar la correcta manipulaci√≥n anal√≠tica posterior.


## **Transformaciones y limpieza de datos**

Durante esta fase se aplicaron transformaciones sobre el DataFrame original con los siguientes objetivos:

* **Eliminaci√≥n de duplicados:**

  ```python
  df = df.dropDuplicates()
  ```

* **Manejo de valores nulos:**
  Se eliminaron registros con datos faltantes en campos clave como *Name* o *Global_Sales*:

  ```python
  df = df.na.drop(subset=["Name", "Global_Sales"])
  ```

* **Creaci√≥n de columnas derivadas:**
  Por ejemplo, se cre√≥ una nueva columna con el total de ventas globales en millones:

  ```python
  df = df.withColumn("Global_Sales_Millions", df["Global_Sales"].cast("double") * 1.0)
  ```

* **Filtrado de datos:**
  Se filtraron los t√≠tulos con ventas globales mayores a 1 mill√≥n para focalizar el an√°lisis en los juegos m√°s exitosos:

  ```python
  df_filtered = df.filter(df["Global_Sales_Millions"] > 1)
  ```

* **Agregaciones:**
  Se realizaron agrupaciones por g√©nero y plataforma para calcular promedios de ventas:

  ```python
  df_grouped = df.groupBy("Genre").agg({"Global_Sales_Millions": "avg"})
  ```

Estas transformaciones permitieron obtener un dataset limpio y coherente, apto para an√°lisis descriptivos y visualizaciones posteriores.

## **Almacenamiento en Delta Lake**

Una vez obtenido el conjunto de datos transformado, se guard√≥ en formato **Delta Lake**, que extiende el formato Parquet incorporando capacidades ACID (Atomicidad, Consistencia, Aislamiento y Durabilidad), control de versiones y lecturas/escrituras incrementales.

El proceso se realiz√≥ con:

```python
df.write.format("delta").mode("overwrite").save("/Volumes/workspace/default/tmp_steam/vgsales_clean_delta")
```

Posteriormente, se ley√≥ nuevamente desde Delta para validar su integridad:

```python
df_delta = spark.read.format("delta").load("/Volumes/workspace/default/tmp_steam/vgsales_clean_delta")
df_delta.show(5)
```

El uso de Delta Lake garantiza **eficiencia en las consultas**, **consistencia en los datos** y la posibilidad de realizar operaciones de **time travel** (recuperaci√≥n de versiones anteriores del dataset), caracter√≠sticas fundamentales en entornos empresariales modernos.

## **Visualizaci√≥n y an√°lisis exploratorio**

En esta etapa se realizaron visualizaciones b√°sicas dentro del notebook, con el objetivo de identificar patrones relevantes.
Por ejemplo:

* Ventas promedio por g√©nero de videojuego.
* Distribuci√≥n de t√≠tulos por plataforma.
* Tendencia de lanzamientos por a√±o.

Las visualizaciones se generaron mediante funciones integradas de Databricks (`display(df_grouped)`) o con librer√≠as externas como **matplotlib** o **seaborn** en Python.
Estas representaciones gr√°ficas permiten comunicar de forma clara los resultados del an√°lisis.

## **Conclusiones**

El desarrollo de este trabajo permiti√≥ aplicar de manera pr√°ctica los conceptos fundamentales de procesamiento de datos en la nube:

* **Databricks** se consolid√≥ como una herramienta potente y accesible para la gesti√≥n de grandes vol√∫menes de informaci√≥n.
* **Apache Spark** demostr√≥ su eficiencia en la lectura, transformaci√≥n y agregaci√≥n de datos distribuidos.
* **Delta Lake** a√±adi√≥ una capa de confiabilidad y rendimiento que lo hace ideal para arquitecturas **Data Lakehouse** modernas.

Desde una perspectiva tem√°tica, el uso de datos del mercado de videojuegos permiti√≥ generar un contexto atractivo y actual, con aplicaciones potenciales en √°reas de **an√°lisis de mercado**, **predicci√≥n de ventas** o **recomendaci√≥n de productos**.
Este tipo de proyectos integra habilidades muy demandadas en el mercado laboral, como el dominio de entornos de big data, la manipulaci√≥n de datos con PySpark y la comprensi√≥n de arquitecturas de datos modernas.
